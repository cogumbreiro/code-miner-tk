# Salento training

## 1-step tutorial

`salento-train.py` trains an API-Usage Model given a Salento Call-Sequence
Dataset.

**Input:** `dataset.tar.bz2`

**Outputs:**
1. `save.tar.bz2`: the compressed Tensorflow API-Usage Model
2. `train.log`: a log-file of training the API-Usage Model

**Example:**
```
$ ls                 # Ensure that we have our input dataset
dataset.tar.bz2
$ ./salento-train.py # Train our API-Usage Model
...

$ ls                 # We now have the generated API-Usage Model
dataset.tar.bz2 save.tar.bz2 save train.log
```

## Configuration file

When it exists, file `train.yaml` can be used instead of command line
options. The command line options will take precedence over the
configuration file.

To generate the defaults for a `train.yaml` file you can run the following
command:
```
$ salento-train.py --print-args > train.yaml
```

# Data cleaning

By default, `salento-train.py` performs a data cleaning stage, where it 
removes *very* uncommon calls (see `salento-filter.py` for more 
information). To *disable* data cleaning use `--skip-data-clean`.

**Default:** runs the data cleaning stage if there is a vocabulary file
or a stop words file.

*Command line:*
```
salento-train.py --skip-clean-data
```

*Configuration:*
```
clean_data: false
```

## Filter low-term count

As part of the data cleaning stage you can filter out any term with an
TF 0.25% (given by `--idf-treshold`).

*Command line:*
```
--filter-low
```

*Configuration:*

```
run_tf: true
```

## Vocabulary file

The vocabulary is the set of acceptable call names. When
the vocabulary is given `salento-train.py` will only accept terms whose
call name is in the vocabulary set.


**Default:** file `vocabs.txt` if it exists.

*Command-line:*
```
--vocabs some_file.txt
```

*Configuration:*
```
vocabs_file: some_file.txt
```

## Stop words

The stop words define which call names to be excluded *after* filtering.

**Default:** file `stopwords.txt` if it exists.

*Command line:*
```
salento-train.py --stop-words-file some_file.txt
```

*Configuration:*
```
stop_words_file: some_file.txt
```

## Only clean data

You can choose to only run the cleaning data stage.

*Command line:*
```
salento-train.py --run-clean-data
```

*Configuration:*
```
run_clean: true
```

# Spliting data

You can split your dataset into training and testing sets with this stage.
By default it will write to dataset `dataset-train.json.bz2` and
`dataset-test.json.bz2`, using the former for training and ignoring the
latter. You can override these with `--train-file` and `--test-file`,
respectively. Additionally, this will use, by default 80% of the dataset
for training; this option can be tuned with command line option
`--split-ratio`.

*Command line:*
```
--split-data
```

*Configuration:*
```
split_data: true
```


# Tips

`salento-train.py` has many parameters that can be queried with the `--help` parameter.
```
$ salento-train.py --help
usage: salento-train.py [-h] [-C DIRNAME] [-i INFILE] [-f ARGS_FILE]
                        [--print-args] [--save-dir SAVE_DIR]
                        [--log-file LOG_FILE] [--config-file CONFIG_FILE]
                        [--backup-file BACKUP_FILE]
                        [--stop-words-file STOP_WORDS_FILE]
                        [--idf-treshold IDF_TRESHOLD] [--dry-run]
                        [--skip-clean-data] [--skip-log] [--skip-backup]
                        [--echo] [--salento-home SALENTO_HOME]
                        [--python-bin PYTHON_BIN] [--keep-temp] [--rm-all]
                        [--rm-temp]

Trains a Salento API-Usage model.
...
```

You can change the working directory with `-C`; all the arguments are relative to this parameter:
```
$ ls output/
dataset.tar.bz2
$ salento-train.py -C output
...
```

**Removing all generated files.** 
To ensure we are back to an initial stage we can invoke parameter 
`--rm-all`. This will remove any files generated by `salento-train.py`.
```
$ salento-train.py --rm-all
Remove 'dataset-clean.json.bz2', 'train.log', 'save', 'save.tar.bz2'? [Y/n]
```

